sequenceDiagram
    Orchestrator->>+UrlManager: urlsToCrawlInStep(stepSize)
    UrlManager-->>-Orchestrator: URLs
    
    par 
        Orchestrator->>+Worker A: crawlChunk(chunkOfUrls)
        Worker A-->>-Orchestrator: Partial Result
    and
    Orchestrator->>+Worker B: crawlChunk(chunkOfUrls)
    Worker B-->>-Orchestrator: Partial Result
    end

    Orchestrator->>UrlManager: add(urlsFromResult)
    Orchestrator->>Repository: save(dataFromResult)
    
    Orchestrator->>UrlManager: markAsVisited(URLsVisitedInThisStep)


===========================================================================================

classDiagram
Url <|-- UrlVisited
UrlVisited o-- VisitState

VisitResult o-- UrlVisited
VisitResult o-- "*" Url
VisitResult o-- Data


class Url{
    urlAddress: String
}

class UrlVisited{
    visitTimestamp: Timestamp
    visitState: VisitState
}

class VisitState{
    <<enumeration>>
    SUCCESS
    FAIL
}


class VisitResult{
    urlVisited: UrlVisited
    childUrl: List[Url]
    extractedData: Data
}

class Data{
    title: String
    introSection: String
    mainArticle: String
    authors: List[String]
    pubDate: Date
    lastModDate: Date
}


===========================================================================================




classDiagram

    Crawler *-- CrawlerContext

    CrawlerContext o-- Logger
    CrawlerContext o-- Repository
    CrawlerContext o-- UrlManager

    CrawlerContext *-- PageVisitor
    UrlManager o-- Url

    class PageVisitor{
        fun: Url => Document
    }

    class Logger{
        <<interface>>
    } 

    class Repository{
        <<interface>>
    }

    class UrlManager{
        <<interface>>
    }

    class PageVisitor{
        <<interface>>
    }

    
    

===========================================================================================

flowchart TD
    S(Štart)
    S -->|Nastaviť| A[Logger] 
    S -->|Inicializovať| B[UrlManager]
    S -->|Pripraviť| G[Adresy na prejdenie]
    S -->|Inicializovať| E[Repozitár]
    S --> L[Limit]

    B --> C{Pokračovať v predošlom behu}
    C -->|Áno| D[urlManager.loadFromFile] 
    
    E -->|Vložiť do| F[CrawlerContext]
    A -->|Vložiť do| F
    D -->|Vložiť do| F
    C -->|Nie| F

    F --> I[Init Crawler]
    G --> I
    L --> I
    

================================= Urlmanager class diagram
classDiagram
    UrlManager <|.. InMemoryUm : implementuje
    UrlManager <|.. PersistedInMemoryUM : implementuje
    InMemoryUm --* PersistedInMemoryUM: obsahuje

class UrlManager{
    <<interface>>
    upsert(toUpsert: Seq[Url]): Unit
    getBatch(batchSize: Int): Seq[Url]
    markAsCrawled(crawled: Seq[UrlVisitRecord]): Unit
    sizeToCrawl(): Long
    sizeOfCrawled(): Long
}

class InMemoryUm{
    queue: Seq[Url]
    crawled: Set[Url]
}

class PersistedInMemoryUM{
    saveToFile(): Unit
    def loadFromFile(): Unit
}

========================================= Class diagram Repository
classDiagram
    Repository <|.. OneCsvRepo : implementuje
    Repository <|.. InMemoryRepo: implementuje
    Repository <|.. FolderCsvRepo : implementuje

class Repository{
    <<interface>>
    saveStep(stepResults: Seq[RepoDTO])
}

class OneCsvRepo{
   targetPath: Path
}

class FolderCsvRepo{
    targetPath: Path
}

class InMemoryRepo{
    mutableResult: mutable.Buffer[RepoDTO]
    getAllSaved(): Seq[RepoDTO]
}

=============================== DomainScraper

classDiagram
    DomainScraper <|.. SmeDomain : implementuje
    SupportedDomains "*" o-- "1" DomainScraper: obsahuje
    DomainFilter <-- SupportedDomains: používa


class DomainScraper{
    <<interface>>
    parseDocument(doc: Document): PageContent
    isUrlInDomain(url: Url): Boolean
}


class SupportedDomains{
    <<singleton>>
    supportedDomains: Seq[DomainScraper]
    apply(): Seq[DomainScraper]
}

class DomainFilter{
    <<singleton>>
    isUrlInSupportedDomains(url: Url): Boolean
    filterSupportedUrls(urls: Seq[Url]): Seq[Url]
    getDomainScraper(url: Url): Option[DomainScraper]
}